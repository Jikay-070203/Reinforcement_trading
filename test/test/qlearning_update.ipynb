{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "import gymnasium as gym\n",
    "from environment import trading_env\n",
    "plt.rc('figure',titleweight='bold',titlesize='large',figsize=(15,6))\n",
    "plt.rc('axes',labelweight='bold',labelsize='large',titleweight='bold',titlesize='large',grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=np.linspace(0,2*np.pi,200)\n",
    "y=2*np.sin(3*seed)+4\n",
    "z=[]\n",
    "for i in range(y.shape[0]-1):\n",
    "    open=y[i]\n",
    "    close=y[i+1]\n",
    "    high=max(open,close)+0.2*np.random.randn(1)[0]\n",
    "    low=min(open,close)-0.2*np.random.randn(1)[0]\n",
    "    z.append([open,high,low,close])\n",
    "z=pd.DataFrame(z,columns=['Open','High','Low','Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ql_agent():\n",
    "\n",
    "    def __init__(self,env,qtable_height=40,qtable_width=3,learning_rate=0.1,discount_value=1,epochs=1000,epsilon=0.99,epsilon_decay=0.99,overflow=0.01):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.discount_value=discount_value\n",
    "        self.epochs=epochs\n",
    "        self.epsilon=epsilon\n",
    "        self.epsilon_decay=epsilon_decay\n",
    "        self.env=env\n",
    "        self.overflow=overflow\n",
    "        self.qtable_height=qtable_height\n",
    "        self.qtable_width=qtable_width\n",
    "    \n",
    "    def create_qtable(self):\n",
    "        # self.qtable_size=(self.qtable_height,self.qtable_width)\n",
    "        # self.qtable_segment_size=(self.env.observation_max*(1+self.overflow)-self.env.observation_min*(1-self.overflow))/np.array(self.qtable_size)\n",
    "        # self.qtable=np.random.uniform(low=-2,high=-1,size=self.qtable_size+(3,))\n",
    "        self.qtable_size=(self.qtable_height,)\n",
    "        self.qtable_segment_size=(self.env.observation_max*(1+self.overflow)-self.env.observation_min*(1-self.overflow))/np.array(self.qtable_size)\n",
    "        self.qtable=np.random.uniform(low=0,high=1,size=self.qtable_size+(3,))\n",
    "        # self.qtable=np.zeros(self.qtable_size+(3,))\n",
    "\n",
    "    def convert_state(self,current_state):\n",
    "        return tuple(((current_state-self.env.observation_min*(1-self.overflow))/self.qtable_segment_size).astype(int))\n",
    "    \n",
    "    def update_qtable(self,reward,action):\n",
    "        current_q_value=self.qtable[self.current_state+(action,)]\n",
    "\n",
    "        if self.new_state is not None:\n",
    "            self.new_state=self.new_state[0]\n",
    "            self.new_state=self.convert_state(self.new_state)\n",
    "            new_q_value=(1-self.learning_rate) * current_q_value+self.learning_rate*reward\n",
    "        else:\n",
    "            new_q_value=(1-self.learning_rate)*current_q_value+self.learning_rate*(reward+self.discount_value*np.max(self.qtable[self.new_state]))\n",
    "        self.qtable[self.current_state + (action,)] = new_q_value\n",
    "\n",
    "    def train(self):\n",
    "        self.create_qtable()\n",
    "        self.action_list=[]\n",
    "        self.portforlio=None\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            print(f'Epoch :{epoch}')\n",
    "            self.current_state,_=self.env.reset()\n",
    "            self.current_state=self.current_state[0]\n",
    "            self.current_state=self.convert_state(self.current_state)\n",
    "            action_list=[]\n",
    "            epoch_reward=0\n",
    "\n",
    "            # while True:\n",
    "            for _ in range(self.env.df.shape[0]-1):\n",
    "                for action in range(3):\n",
    "                    self.new_state,reward,terminate,truncate,_=self.env.step(action)\n",
    "                    self.update_qtable(reward,action)\n",
    "\n",
    "                self.current_state=self.new_state\n",
    "                epoch_reward+=reward\n",
    "            self.epsilon*=self.epsilon_decay\n",
    "            print(self.env.coin,self.env.usd)\n",
    "            # print(f'Epoch reward :{round(epoch_reward,3)},portforlio valuation :{round(self.portforlio,3)},number of actions :{len(action_list)}')\n",
    "\n",
    "    def visualize(self):\n",
    "        fg=plt.figure()\n",
    "        ax=fg.add_subplot()\n",
    "        self.env.df['Close'].plot(ax=ax)\n",
    "        for i in range(len(self.action_list)):\n",
    "            if self.action_list[i]==0:\n",
    "                plt.text(i,self.env.df.iloc[i,0],'B',color='C2')\n",
    "            elif self.action_list[i]==2:\n",
    "                plt.text(i,self.env.df.iloc[i,0],'S',color='C3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=trading_env(df=z,window_size=1)\n",
    "agent=ql_agent(env,epochs=1000,qtable_height=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
